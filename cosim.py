import os
import re
import torch
import string
import pandas as pd
import numpy as np
from collections import OrderedDict
from transformers import BertTokenizer
from transformers import BertModel
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr, spearmanr
from nltk.corpus import wordnet as wn

####
# The code works like this:
# It will load the model and read the file and load the file at the start. 
# Function auxillary_finder will find the embeddings of target word in top-k
# senses 
# sense_finder will find the closest sense and return the mean representation
# filtered_token_pos will extract the target word and rmmove <strong> tag 
# BERTembedd will generate the similarity score 
# collect_sim will collect the similarity score for all the context 
# correlation_coeff will generate the final score after correlation
# Process will start the pipeline 
####

# To set up the mutiprocessing capability from standard 4 to CPU threads
torch.set_num_threads(os.cpu_count())

# Intialize the models
model_name = 'bert-large-uncased'
comsimlex_path = '/home/shivani/Downloads/A Resource for Evaluating Graded Word Similarity in Context_ CoSimLex/cosimlex_en.csv'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name,output_hidden_states=True)
cosim = pd.read_csv(comsimlex_path, sep='\t')

#Collect the gold standars similarity 
sim_all_gold = np.array(cosim['sim1'].to_list() + cosim['sim2'].to_list())

def auxillary_finder(word):
    '''
    It will take a word and number of sense to consider. It will return 
    embedding of traget word in all the sense top -k sense
    '''
    global k
    # print("Sense to be considered {}".format(k))
    all_tensor = []
    for i in wn.synsets(word)[:k]:
        sentence = word + ' is ' + (i.definition())
        outputs, y = BERTembedd(sentence, [0], only_embed=True)
        hidden_states = outputs[2]
        token_embeddings = torch.stack(hidden_states, dim=0)
        token_embeddings.size()
        token_embeddings = torch.squeeze(token_embeddings, dim=1)
        f_v = torch.mean(token_embeddings[-4:], dim=0)
#         print (y[1]['pos'])
        final_tensor = torch.mean(f_v[y[1]['pos']], dim=0)
        all_tensor.append(final_tensor)
    return all_tensor 


def sense_finder(word, original_embedding):
    '''
    It will produce the mean of the original vector and sense vector which is nearest to the
    original vector.
    '''
    all_tensor = auxillary_finder(word)
    if len(all_tensor) == 0:
        return original_embedding
    else:
        cdist = []
        for sense_embed in all_tensor:
            cdist.append(cosine(sense_embed, original_embedding))
        cdist = np.array(cdist)
        lowest_cdist_idx = np.argsort(cdist)[0]
        # print (cdist, lowest_cdist_idx)
        return torch.mean(torch.stack([all_tensor[lowest_cdist_idx], original_embedding]), dim=0)


def filtered_token_pos(context):
    """
    It will take a context and filter the word which are target word and return the indexes of 
    both the target words.
    """
    key_pos = []
    key = []
    for idx, i in enumerate(context.split()):
        if i.startswith('<'):
            key.append(re.findall('>(\w+)<', i)[0])
            key_pos.append(idx)
        else:
            key.append(i)
    #         print(key, key_pos)
    text = ' '.join(key)
    return key, key_pos


# In[8]:


def BERTembedd(text, key_pos, only_embed=False):
    """
    It will take the context and key position generated by function filtered_token_pos. 
    It will work in two model if only_embed flag is True then it will not generate similarity
    Finally, after recieving both average disambiguated word, it will generate cosine distance 
    between both the target words. 
    """
    marked_text = "[CLS] " + text + " [SEP]"
    # Split the sentence into tokens.
    tokenized_text = tokenizer.tokenize(marked_text)
    mapping = OrderedDict()
    start = 0
    for idx, i in enumerate(marked_text.split()):
       token = tokenizer.tokenize(i)
       mapping[idx] = {'token' : token, 
                    'pos' : np.arange(start, start+len(token)), 
                      'oword' : i}
       start += len(token)

    final_mask = []
    for i in mapping.items():
       final_mask += (i[1]['token'])

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = tokenizer.convert_tokens_to_ids(final_mask)
    tokens_tensor = torch.tensor([indexed_tokens])
    tokens_tensor

    #Run the text through BERT, get the output and collect all of the hidden states produced from all 12 layers.
    with torch.no_grad():
          outputs = model(tokens_tensor)                    
    if only_embed:
        return outputs, mapping
    hidden_states = outputs[2]
    token_embeddings = torch.stack(hidden_states, dim=0)  
#     print(token_embeddings.shape)
    token_embeddings = torch.squeeze(token_embeddings[-4:], dim=1)
    f_v = torch.mean(token_embeddings, dim=0)

    # Process and find the closest sense. Collect the averaged tensor.
    new_t0 = sense_finder(mapping[key_pos[0]+1]['oword'] , torch.mean(f_v[mapping[key_pos[0]+1]['pos']], dim=0))
    new_t1 = sense_finder(mapping[key_pos[1]+1]['oword'] , torch.mean(f_v[mapping[key_pos[1]+1]['pos']], dim=0))

    # Generate the similarity score separately for the original target words and sense enriched target words
    similarity = cosine(new_t0, new_t1)
    similarity_ = cosine(torch.mean(f_v[mapping[key_pos[0]+1]['pos']], dim=0),
                    torch.mean(f_v[mapping[key_pos[1]+1]['pos']], dim=0))

    return similarity, similarity_

def collect_sim(context='c1'):
    assert context == 'c1' or context == 'c2'
    sim_val_c1 = []
    sim_val_orig_c1 = []
    gold_data = []
    for idx, row in enumerate(cosim.itertuples()):
        if context == 'c1':
            val = row.context1.lower()
            sim_score = row.sim1
        else:
            val = row.context2.lower()
            sim_score = row.sim2
        # val = val.translate(str.maketrans('', '', string.punctuation))
        # print(val)
        key, key_pos = filtered_token_pos(val)
        if len(key_pos) != 2:
            # sim = 0.5
            # sim_=0.5
            # print(val)
            pass
            # print("Skipping")
        else:
            sim, sim_ = BERTembedd(' '.join(key), key_pos)
            sim_val_c1.append(round(sim, 4))
            sim_val_orig_c1.append(round(sim_, 4))
            gold_data.append(sim_score)
    return sim_val_c1, sim_val_orig_c1, gold_data

def correlation_coeff(score, gold_data):
    """
    It will take the list of similarity score and calculates pearson correlation and 
    spearman correlation. Finally, it returns the score as per competition standard.
    """
    a = spearmanr(score, gold_data)[0]
    b = pearsonr(score, gold_data)[0]
    return (2*a * b )/(a+b)


def process():
    print ("Starting to process Context 1")
    sim_val_c1, sim_val_orig_c1, g_c1 = collect_sim()
    sim_val_c2, sim_val_orig_c2, g_c2 = collect_sim('c2')
    print ("Starting to process Context 2")
    sim_val = sim_val_c1 + sim_val_c2
    sim_val_orig = sim_val_orig_c1 + sim_val_orig_c2
    gold_data = g_c1 +g_c2

    # Since the generated value is cosine distance. Converting it to cosine similarity
    sim_val_new = 1- np.array(sim_val)
    sim_val_old = 1 -np.array(sim_val_orig)
    return correlation_coeff(sim_val_new, gold_data), correlation_coeff(sim_val_old, gold_data)


# Tabulation of results
if __name__ == '__main__':
    for i in range(1,8):
        k = i
        x , y = process()
        print("Modified val {} and original BERT {} for {}".format(x, y, k))

